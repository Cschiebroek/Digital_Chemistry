{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks serves as an example on how to run the model, by splitting the training into train and validation, and see how well it does on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data import load_data,scale_props,get_graphs\n",
    "from utils_plotting import plot_property_histograms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_model import train_and_validate_multi,get_preds_per_task,preds_and_ys_to_df\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.models import AttentiveFP\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously created graphs\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train_graphs_DASH_charge_scaled = get_graphs(train,dash_charges=True,scaled =True,save_graphs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14686 3672\n",
      "Epoch 1/100: Train Loss: 0.1271, Val Loss: 0.0685\n",
      "Epoch 2/100: Train Loss: 0.0663, Val Loss: 0.0668\n",
      "Epoch 3/100: Train Loss: 0.0597, Val Loss: 0.0650\n",
      "Epoch 4/100: Train Loss: 0.0579, Val Loss: 0.0595\n",
      "Epoch 5/100: Train Loss: 0.0579, Val Loss: 0.0573\n",
      "Epoch 6/100: Train Loss: 0.0557, Val Loss: 0.0635\n"
     ]
    }
   ],
   "source": [
    "outputs = 14 #keep this at 14 for now, this is all the properties for which graphs are made\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_data, val_data = train_test_split(train_graphs_DASH_charge_scaled, test_size=0.2, random_state=2000)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "model= AttentiveFP(in_channels=24, hidden_channels=200, out_channels=outputs, #note that using the DASH graphs will increase the amount of node feauters (input channels for model) from 23 to 24\n",
    "                            edge_dim=11, num_layers=4, num_timesteps=2,\n",
    "                            dropout=0.0).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=10**-3,\n",
    "                        weight_decay=10**-4)\n",
    "print(len(train_data), len(val_data))\n",
    "train_and_validate_multi(model, train_loader, val_loader, optimizer, num_epochs=100, outputs=outputs, verbose=True,props_to_train=['LogVP','LogP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, ys = get_preds_per_task(model,val_loader, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make df from preds\n",
    "model= AttentiveFP(in_channels=24, hidden_channels=200, out_channels=outputs,\n",
    "                        edge_dim=11, num_layers=4, num_timesteps=2,\n",
    "                        dropout=0.0).to(device)\n",
    "model.load_state_dict(torch.load('test_model.pt'))\n",
    "#load the scaler we used for the original training data\n",
    "import pickle\n",
    "scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
    "df_preds,df_ys = preds_and_ys_to_df(preds,ys,ref_df = train,scaler = scaler,props_to_train=['LogVP','LogP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "from utils_plotting import plot_scatters\n",
    "plot_scatters(df_preds,df_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "1. Add the different endpoints - check\n",
    "2. Normalization of values\n",
    "3. Weighting of the tasks\n",
    "4. Hyperparameter optimization\n",
    "5. Benchmarking datasets? E.g. SAMPL7 for logP\n",
    "6. Quadruple check refs - check\n",
    "7. Think about datasplitting: same splits as OPERA? possible? otherwise, just cross-val? or random splits (multiple)\n",
    "8. Different random weight initializations of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split: 80/20 5x\n",
    "\n",
    "HP OPT: https://docs.ray.io/en/latest/tune/api/doc/ray.tune.search.bayesopt.BayesOptSearch.html\n",
    "Repeated K-fold: https://greglandrum.github.io/rdkit-blog/posts/2023-08-13-xval-variability1.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
