{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import os\n",
    "from math import sqrt\n",
    "import pandas\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#torch stuff\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch_geometric.nn.models import AttentiveFP\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "#stats\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "#HP tuning\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import tune,train\n",
    "\n",
    "#custom\n",
    "from utils_data_prep import Prep_Graphs\n",
    "from utils_plotting import plot_multiple_seeds\n",
    "from utils_stats import perform_analysis\n",
    "\n",
    "#other\n",
    "from functools import partial\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#set random seeds\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "def get_path(sandbox):\n",
    "    if sandbox:\n",
    "        path = 'sandbox/'\n",
    "    else:\n",
    "        path = ''\n",
    "    return path\n",
    "\n",
    "\n",
    "#Direct Learning\n",
    "def hp_finder(config,data_train,data_val_1,num_epochs=50):\n",
    "    train_loader = DataLoader(data_train, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(data_val_1, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    model = AttentiveFP(in_channels=23, hidden_channels=config[\"hidden_channels\"], out_channels=1,\n",
    "                        edge_dim=11, num_layers=config[\"num_layers\"], num_timesteps=config[\"num_timesteps\"],\n",
    "                        dropout=0.0).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"],\n",
    "                                weight_decay=10**-5)\n",
    "    if config[\"Scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=2, factor=config[\"gamma\"],verbose=False)\n",
    "    elif config[\"Scheduler\"] == \"ExponentialLR\":\n",
    "        scheduler = ExponentialLR(optimizer, gamma=config[\"gamma\"])\n",
    "    else:\n",
    "        raise ValueError('Scheduler not found')\n",
    "\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = train_func(train_loader, model, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = val_func(val_loader, model)\n",
    "        val_losses.append(val_loss)\n",
    "        if config[\"Scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        ys,preds = get_preds(val_loader, model)\n",
    "        k_tau = kendalltau(ys,preds).statistic\n",
    "        train.report({\"kendall_tau\": k_tau})\n",
    "\n",
    "def train_func(train_loader, model, optimizer):\n",
    "    total_loss  = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        data.y = data.y.view(-1, 1)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "        del data\n",
    "\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "\n",
    "def val_func(val_loader, model):\n",
    "    total_loss = total_examples = 0\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        data.y = data.y.view(-1, 1)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "        del data\n",
    "\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "def get_preds(val_loader, model):\n",
    "    preds,ys = [],[]\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        preds.extend(out)\n",
    "        ys.extend(data.y)\n",
    "    preds = [float(p) for p in preds]\n",
    "    ys = [float(y) for y in ys]         \n",
    "    print(len(ys))\n",
    "    return preds,ys\n",
    "\n",
    "def hp_serach_direct(data_train,data_val):\n",
    "    data_train = data_train\n",
    "    data_val = data_val\n",
    "    search_space = {\n",
    "        #training params\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"batch_size\": tune.choice([8, 16, 32]),\n",
    "        #model params\n",
    "        \"hidden_channels\": tune.choice([100, 200, 300]),\n",
    "        \"num_layers\": tune.choice([2, 3, 4]),\n",
    "        \"num_timesteps\": tune.choice([1,2,3]),\n",
    "        #scheduler params\n",
    "        \"gamma\": tune.loguniform(0.9, 0.99),\n",
    "        \"Scheduler\": tune.choice([\"ReduceLROnPlateau\", \"ExponentialLR\"])\n",
    "\n",
    "    }\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        partial(hp_finder, data_train=data_train, data_val_1=data_val),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=100,\n",
    "            scheduler=ASHAScheduler(metric=\"kendall_tau\", mode=\"max\"),\n",
    "        ),\n",
    "        param_space=search_space,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    kendall_tau = 0\n",
    "    dfs = {result.path: result.metrics_dataframe for result in results}\n",
    "\n",
    "    for d in dfs.values():\n",
    "        if max(d['kendall_tau']) > kendall_tau:\n",
    "            kendall_tau = max(d['kendall_tau'])\n",
    "            best_trial = d\n",
    "\n",
    "    # #return best params\n",
    "    # lr = max(d['config/lr'])\n",
    "    # batch_size = max(d['config/batch_size'])\n",
    "    # hidden_channels = max(d['config/hidden_channels'])\n",
    "    # num_layers = max(d['config/num_layers'])\n",
    "    # num_timesteps = max(d['config/num_timesteps'])\n",
    "    # gamma = max(d['config/gamma'])\n",
    "    # Scheduler = max(d['config/Scheduler'])\n",
    "\n",
    "    return best_trial\n",
    "\n",
    "#now production run\n",
    "def train_direct(train_loader, model, optimizer):\n",
    "    total_loss  = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        data.y = data.y.view(-1, 1)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "def val_direct(train_loader, model):\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.y = data.y.view(-1, 1)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "\n",
    "def train_val_direct(model, train_loader, val_loader, optimizer, num_epochs,tid,assay_type,weight_seed,split_seed,verbose=True,sandbox=False,run_name='default',finetune_lr=None):\n",
    "    path = get_path(sandbox)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=2, factor=0.9,verbose=False)\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    min_val_los = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = train_direct(train_loader, model, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = val_direct(val_loader, model)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < min_val_los:\n",
    "            min_val_los = val_loss\n",
    "            counter = 0\n",
    "            if finetune_lr:\n",
    "                torch.save(model.state_dict(), path+f'models/model_state_dict_{tid}_{assay_type}_single_{weight_seed}_{split_seed}_finetune_lr_{finetune_lr}_{run_name}.pt')\n",
    "            else:\n",
    "                torch.save(model.state_dict(), path+f'models/model_state_dict_{tid}_{assay_type}_single_{weight_seed}_{split_seed}_{run_name}.pt')\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter > 10:\n",
    "            if verbose:\n",
    "                print('early stopping')\n",
    "            break\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
