{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from serenityff.charge.tree.dash_tree import DASHTree\n",
    "from serenityff.charge.tree.atom_features import AtomFeatures\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import sys\n",
    "#add path to custom_featurization_stuff\n",
    "sys.path.append(\"/localhome/cschiebroek/MDFPs/mdfptools/carl/Experiments_result_analysis/\")\n",
    "from custom_featurization_stuff import get_graph_from_mol\n",
    "allowable_set= [\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\",\"H\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "#also add r2\n",
    "from sklearn.metrics import r2_score\n",
    "#also add pearson\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:56:26] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 12 22 23 24 25 26 27\n",
      "[15:56:26] ERROR: Could not sanitize molecule ending on line 38202\n",
      "[15:56:26] ERROR: Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 12 22 23 24 25 26 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>logP</th>\n",
       "      <th>logVP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCCCCCC1CCCCC1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.3978998382903127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)N(NC(=O)c1ccccc1)C(=O)c1ccccc1Cl</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(=O)Nc1ccc(Cl)c(C(F)(F)F)c1</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCc1cc(OP(=S)(OC)OC)ccc1[N+](=O)[O-]</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC(=O)c1cccc([N+](=O)[O-])c1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15075</th>\n",
       "      <td>CC(C)=CC1C(C(=O)OCc2cccc(I)c2)C1(C)C</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>N#Cc1c[nH]cc1-c1cccc2c1OC(F)(F)O2</td>\n",
       "      <td>4.12</td>\n",
       "      <td>-8.533099771728077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15077</th>\n",
       "      <td>CN(C)CCOC(=O)c1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15078</th>\n",
       "      <td>O=C(Br)c1ccccc1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6536000773322567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15079</th>\n",
       "      <td>Clc1cc(Cl)cc(-c2ccccc2)c1</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15080 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         SMILES  logP                logVP\n",
       "0                               CCCCCCCC1CCCCC1   0.0  -1.3978998382903127\n",
       "1      CC(C)(C)N(NC(=O)c1ccccc1)C(=O)c1ccccc1Cl  2.59                  0.0\n",
       "2                 CC(=O)Nc1ccc(Cl)c(C(F)(F)F)c1  3.33                  0.0\n",
       "3          CCc1cc(OP(=S)(OC)OC)ccc1[N+](=O)[O-]  3.74                  0.0\n",
       "4                  NC(=O)c1cccc([N+](=O)[O-])c1  0.77                  0.0\n",
       "...                                         ...   ...                  ...\n",
       "15075      CC(C)=CC1C(C(=O)OCc2cccc(I)c2)C1(C)C  6.61                  0.0\n",
       "15076         N#Cc1c[nH]cc1-c1cccc2c1OC(F)(F)O2  4.12   -8.533099771728077\n",
       "15077       CN(C)CCOC(=O)c1ccc([N+](=O)[O-])cc1   1.7                  0.0\n",
       "15078                           O=C(Br)c1ccccc1   0.0  -0.6536000773322567\n",
       "15079                 Clc1cc(Cl)cc(-c2ccccc2)c1  5.41                  0.0\n",
       "\n",
       "[15080 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start with loading the data\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "\n",
    "# Function to read SDF file and extract properties\n",
    "def read_sdf(file_path):\n",
    "    suppl = Chem.SDMolSupplier(file_path)\n",
    "    data = []\n",
    "    for mol in suppl:\n",
    "        if mol is not None:\n",
    "            smiles = Chem.MolToSmiles(mol)\n",
    "            logP = mol.GetProp('LogP') if mol.HasProp('LogP') else None\n",
    "            logVP = mol.GetProp('LogVP') if mol.HasProp('LogVP') else None\n",
    "            data.append({'SMILES': smiles, 'logP': logP, 'logVP': logVP})\n",
    "    return data\n",
    "\n",
    "# File paths for your SDF files\n",
    "vp_path = 'OPERA_Data/VP_QR.sdf'\n",
    "logp_path = 'OPERA_Data/LogP_QR.sdf'\n",
    "\n",
    "# Read SDF files and extract data\n",
    "data_vp = read_sdf(vp_path)\n",
    "data_logp = read_sdf(logp_path)\n",
    "\n",
    "# Create Pandas dataframes\n",
    "df_vp = pd.DataFrame(data_vp)\n",
    "df_logp = pd.DataFrame(data_logp)\n",
    "\n",
    "#get the smiles for both\n",
    "smiles_vp = df_vp['SMILES'].tolist()\n",
    "smiles_logp = df_logp['SMILES'].tolist()\n",
    "#get combined smiles\n",
    "smiles_all = smiles_vp + smiles_logp\n",
    "smiles_all = [s for s in smiles_all if s is not None]\n",
    "smiles_all = list(set(smiles_all))\n",
    "#foreach smile in overlap, get the logP from df2 and the logVP from df1\n",
    "logP = []\n",
    "logVP = []\n",
    "for smile in smiles_all:\n",
    "    #append val or 0.0 if not found\n",
    "    logP.append(df_logp[df_logp['SMILES'] == smile]['logP'].values[0] if smile in smiles_logp else 0.0)\n",
    "    logVP.append(df_vp[df_vp['SMILES'] == smile]['logVP'].values[0] if smile in smiles_vp else 0.0)\n",
    "    \n",
    "#make a new dataframe\n",
    "df = pd.DataFrame({'SMILES': smiles_all, 'logP': logP, 'logVP': logVP})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do histograms of the logps and vps\n",
    "logp_vals = df['logP'].tolist()\n",
    "vp_vals = df['logVP'].tolist()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.histplot(logp_vals, ax=ax[0])\n",
    "ax[0].set_title('logP')\n",
    "sns.histplot(vp_vals, ax=ax[1])\n",
    "ax[1].set_title('logVP')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DASH tree data\n"
     ]
    }
   ],
   "source": [
    "tree = DASHTree(tree_folder_path='/localhome/cschiebroek/other/serenityff-charge/tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15079/15079 [05:28<00:00, 45.88it/s]\n"
     ]
    }
   ],
   "source": [
    "all_mols = [Chem.MolFromSmiles(smi) for smi in df['SMILES']]\n",
    "mols = [m for m in all_mols if m.GetNumAtoms() > 1]\n",
    "error_mol = [m for m in all_mols if m.GetNumAtoms() <= 1]\n",
    "mols_with_charges = []\n",
    "error_mols_charges = []\n",
    "for m in tqdm(mols):\n",
    "    try:\n",
    "        mol = Chem.AddHs(m, addCoords=True)\n",
    "        charges = tree.get_molecules_partial_charges(mol,chg_std_key='std',chg_key='result')[\"charges\"]\n",
    "    except:\n",
    "        error_mols_charges.append(m)\n",
    "        continue\n",
    "    for i,atom in enumerate(mol.GetAtoms()):\n",
    "        atom.SetDoubleProp('charge',charges[i])\n",
    "    mols_with_charges.append(mol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 1 142\n"
     ]
    }
   ],
   "source": [
    "indices_to_drop_size = [all_mols.index(m) for m in error_mol]\n",
    "indices_to_drop_charges = [all_mols.index(m) for m in error_mols_charges]\n",
    "indices_to_drop_total = list(set(indices_to_drop_size + indices_to_drop_charges))\n",
    "print(len(indices_to_drop_total), len(indices_to_drop_size), len(indices_to_drop_charges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if indices_to_drop_total:\n",
    "    print('Caution! Mols dropped')\n",
    "    df = df.drop(indices_to_drop_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set values of second two columns as floats\n",
    "df['logP'] = df['logP'].astype(float)\n",
    "df['logVP'] = df['logVP'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = df.iloc[:, -2:].values\n",
    "y = torch.tensor(ys, dtype=torch.float32)\n",
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -1.3979]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomData(x=[39, 24], edge_index=[2, 78], edge_attr=[78, 11], y=[39], batch=[39], molecule_charge=[1], smiles='[H]C([H])([H])C([H])([H])C([H])([H])C([H])([H])C([H])([H])C([H])([H])C([H])([H])C1([H])C([H])([H])C([H])([H])C([H])([H])C([H])([H])C1([H])[H]', sdf_idx=0)\n"
     ]
    }
   ],
   "source": [
    "assert len(mols_with_charges) == len(y)\n",
    "data = [get_graph_from_mol(mol,i, allowable_set,no_y=True) for i,mol in enumerate(mols_with_charges)]\n",
    "for i in range(len(data)):\n",
    "    data[i].y = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import os\n",
    "from math import sqrt\n",
    "import pandas\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#torch stuff\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch_geometric.nn.models import AttentiveFP\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#set random seeds\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "def train_multi(train_loader, model, optimizer, device, outputs):\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        weighted_loss = num_labels = 0\n",
    "        for i in range(outputs):\n",
    "            y_tmp = data.y[:, i]\n",
    "            out_tmp = out[:, i]\n",
    "            # Find indices where labels are available\n",
    "            present_label_indices = torch.nonzero(y_tmp != 0.0).view(-1)\n",
    "            num_labels += len(present_label_indices)\n",
    "\n",
    "            if len(present_label_indices) > 0:\n",
    "                # Extract only the available label indices\n",
    "                out_tmp_present = torch.index_select(out_tmp, 0, present_label_indices)\n",
    "                y_tmp_present = torch.index_select(y_tmp, 0, present_label_indices)\n",
    "\n",
    "                # Calculate MSE loss only for available labels\n",
    "                loss_tmp_present = F.mse_loss(out_tmp_present, y_tmp_present)\n",
    "                if not torch.isnan(loss_tmp_present):\n",
    "                    weighted_loss += loss_tmp_present * len(present_label_indices)\n",
    "\n",
    "        weighted_loss = weighted_loss / num_labels\n",
    "        weighted_loss.backward()\n",
    "\n",
    "        total_loss += float(weighted_loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "        # clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        del data\n",
    "\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "\n",
    "def validate_multi(val_loader, model, outputs):\n",
    "    total_loss = total_examples = 0\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        weighted_loss = num_labels = 0\n",
    "        for i in range(outputs):\n",
    "\n",
    "            y_tmp = data.y[:, i]\n",
    "            out_tmp = out[:, i]\n",
    "            # Find indices where labels are available\n",
    "            present_label_indices = torch.nonzero(y_tmp != 0.0).view(-1)\n",
    "            num_labels += len(present_label_indices)\n",
    "\n",
    "            if len(present_label_indices) > 0:\n",
    "                # Extract only the available label indices\n",
    "                out_tmp_present = torch.index_select(out_tmp, 0, present_label_indices)\n",
    "                y_tmp_present = torch.index_select(y_tmp, 0, present_label_indices)\n",
    "\n",
    "                # Calculate MSE loss only for available labels\n",
    "                loss_tmp_present = F.mse_loss(out_tmp_present, y_tmp_present)\n",
    "                if not torch.isnan(loss_tmp_present):\n",
    "                    weighted_loss += loss_tmp_present * len(present_label_indices)\n",
    "\n",
    "        weighted_loss = weighted_loss / num_labels\n",
    "\n",
    "        total_loss += float(weighted_loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "        del data\n",
    "\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "def train_and_validate_multi(model, train_loader, val_loader, optimizer, num_epochs, outputs, verbose=True):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=2, factor=0.9,verbose=False)\n",
    "\n",
    "    min_val_los = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = train_multi(train_loader, model, optimizer, device, outputs)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = validate_multi(val_loader, model, outputs)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < min_val_los:\n",
    "            min_val_los = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'test_model.pt')\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter > 10:\n",
    "            if verbose:\n",
    "                print('early stopping')\n",
    "            break\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss: 1.5453, Val Loss: 1.0815\n",
      "Epoch 2/100: Train Loss: 1.0209, Val Loss: 0.9635\n",
      "Epoch 3/100: Train Loss: 0.8893, Val Loss: 0.8559\n",
      "Epoch 4/100: Train Loss: 0.8236, Val Loss: 0.8044\n",
      "Epoch 5/100: Train Loss: 0.7990, Val Loss: 0.8357\n",
      "Epoch 6/100: Train Loss: 0.7630, Val Loss: 0.7707\n",
      "Epoch 7/100: Train Loss: 0.7258, Val Loss: 0.7408\n",
      "Epoch 8/100: Train Loss: 0.6903, Val Loss: 0.7275\n",
      "Epoch 9/100: Train Loss: 0.6593, Val Loss: 0.7081\n",
      "Epoch 10/100: Train Loss: 0.6488, Val Loss: 0.7363\n",
      "Epoch 11/100: Train Loss: 0.6417, Val Loss: 0.6921\n",
      "Epoch 12/100: Train Loss: 0.6216, Val Loss: 0.7320\n",
      "Epoch 13/100: Train Loss: 0.6229, Val Loss: 0.7218\n",
      "Epoch 14/100: Train Loss: 0.5946, Val Loss: 0.6843\n",
      "Epoch 15/100: Train Loss: 0.5785, Val Loss: 0.6894\n",
      "Epoch 16/100: Train Loss: 0.5756, Val Loss: 0.6978\n",
      "Epoch 17/100: Train Loss: 0.5671, Val Loss: 0.7185\n",
      "Epoch 18/100: Train Loss: 0.5562, Val Loss: 0.6721\n",
      "Epoch 19/100: Train Loss: 0.5526, Val Loss: 0.6648\n",
      "Epoch 20/100: Train Loss: 0.5334, Val Loss: 0.6658\n",
      "Epoch 21/100: Train Loss: 0.5282, Val Loss: 0.6504\n",
      "Epoch 22/100: Train Loss: 0.5183, Val Loss: 0.7006\n",
      "Epoch 23/100: Train Loss: 0.5237, Val Loss: 0.6594\n",
      "Epoch 24/100: Train Loss: 0.5029, Val Loss: 0.6418\n",
      "Epoch 25/100: Train Loss: 0.5098, Val Loss: 0.6188\n",
      "Epoch 26/100: Train Loss: 0.5095, Val Loss: 0.6474\n",
      "Epoch 27/100: Train Loss: 0.4901, Val Loss: 0.6153\n",
      "Epoch 28/100: Train Loss: 0.4983, Val Loss: 0.6098\n",
      "Epoch 29/100: Train Loss: 0.4847, Val Loss: 0.6191\n",
      "Epoch 30/100: Train Loss: 0.4768, Val Loss: 0.6155\n",
      "Epoch 31/100: Train Loss: 0.4695, Val Loss: 0.6272\n",
      "Epoch 32/100: Train Loss: 0.4597, Val Loss: 0.6325\n",
      "Epoch 33/100: Train Loss: 0.4575, Val Loss: 0.6136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=0)\n",
    "#split train in train and val\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=0)\n",
    "# #datloaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "model= AttentiveFP(in_channels=24, hidden_channels=200, out_channels=2,\n",
    "                            edge_dim=11, num_layers=4, num_timesteps=2,\n",
    "                            dropout=0.0).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=10**-3,\n",
    "                        weight_decay=10**-4)\n",
    "train_and_validate_multi(model, train_loader, val_loader, optimizer, num_epochs=100, outputs=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr,kendalltau\n",
    "def get_preds_per_task(val_loader, model, outputs):\n",
    "    preds = tuple([[] for i in range(outputs)])\n",
    "    ys = tuple([[] for i in range(outputs)])\n",
    "    counter = 0\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        for i in range(outputs):\n",
    "            y_tmp = data.y[:, i]\n",
    "            out_tmp = out[:, i]\n",
    "\n",
    "            # Find indices where labels are available\n",
    "            present_label_indices = torch.nonzero(y_tmp != 0).view(-1)\n",
    "\n",
    "            if len(present_label_indices) > 0:\n",
    "                counter += len(present_label_indices)\n",
    "                # Extract only the available label indices\n",
    "                out_tmp_present = torch.index_select(out_tmp, 0, present_label_indices)\n",
    "                y_tmp_present = torch.index_select(y_tmp, 0, present_label_indices)\n",
    "                out_tmp_present = [float(i) for i in out_tmp_present]\n",
    "                y_tmp_present = [float(i) for i in y_tmp_present]\n",
    "                preds[i].extend(out_tmp_present)  \n",
    "                ys[i].extend(y_tmp_present)\n",
    "    print(counter)\n",
    "    return preds,ys\n",
    "\n",
    "\n",
    "def get_stats(out_list,y_list):\n",
    "    kendall_tau = kendalltau(y_list, out_list)[0]\n",
    "    rmse_overall = np.sqrt(np.mean((np.array(y_list) - np.array(out_list)) ** 2))\n",
    "    mae_overall = np.mean(np.abs(np.array(y_list) - np.array(out_list)))\n",
    "    within_03_overall = np.mean(np.abs(np.array(y_list) - np.array(out_list)) < 0.3)\n",
    "    within_1_overall = np.mean(np.abs(np.array(y_list) - np.array(out_list)) < 1)\n",
    "\n",
    "    return kendall_tau,rmse_overall,mae_overall,within_03_overall,within_1_overall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not plot the outputs in same way, but for each task (colored)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "preds,ys = get_preds_per_task(test_loader, model, 6)\n",
    "#get the names of the columns\n",
    "column_names = df.columns[-6:]\n",
    "\n",
    "for i in range(2):\n",
    "    kendall_tau,rmse_overall,mae_overall,within_03_overall,within_1_overall = get_stats(preds[i],ys[i])\n",
    "    r2 = r2_score(ys[i],preds[i])\n",
    "    pearson = pearsonr(ys[i],preds[i]).statistic\n",
    "    print('Pearson:',pearson)\n",
    "    print('R2:',r2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(ys[i], preds[i], alpha=0.5)\n",
    "    min_val = min(min(ys[i]),min(preds[i]))\n",
    "    max_val = max(max(ys[i]),max(preds[i]))\n",
    "    #add diagonal, and lines at +- 0.3 and +- 1\n",
    "    plt.plot([min_val-1,max_val+1],[min_val-1,max_val+1],color='black')\n",
    "    plt.plot([min_val-1,max_val+1],[min_val-1+0.3,max_val+1+0.3],color='black',linestyle='--')\n",
    "    plt.plot([min_val-1,max_val+1],[min_val-1-0.3,max_val+1-0.3],color='black',linestyle='--')\n",
    "    plt.plot([min_val-1,max_val+1],[min_val-1+1,max_val+1+1],color='black',linestyle='--')\n",
    "    plt.plot([min_val-1,max_val+1],[min_val-1-1,max_val+1-1],color='black',linestyle='--')\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(column_names[i])\n",
    "    print(f'kendall_tau: {kendall_tau}')\n",
    "    print(f'rmse_overall: {rmse_overall}')\n",
    "    print(f'mae_overall: {mae_overall}')\n",
    "    print(f'within_03_overall: {within_03_overall}')\n",
    "    print(f'within_1_overall: {within_1_overall}')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
