{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 19:15:58,793 - INFO - Enabling RDKit 2023.09.5 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "#import: standard libraries, plus the classes\n",
    "from modules.utils_model import SingleTaskModelTrainer, MultiTaskModelTrainer\n",
    "from ray import tune\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('modules')\n",
    "from utils_data import get_graphs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously created scaler\n",
      "Loading previously created graphs\n"
     ]
    }
   ],
   "source": [
    "#start with getting the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train_graphs_DASH_charge_scaled = get_graphs(train,dash_charges=True,scaled =True,save_graphs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to split double: we use the first validation set to tune our hyperparameters, and then a second one to be used for early stopping of the final model. We could have smaller sets I think\n",
    "train_data, val_data = train_test_split(train_graphs_DASH_charge_scaled, test_size=0.2, random_state=2000)\n",
    "val1_data, val2_data = train_test_split(val_data, test_size=0.5, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 19:16:17,367 - INFO - Creating model example_MTL_seed_18012000\n"
     ]
    }
   ],
   "source": [
    "example_mtl_model = MultiTaskModelTrainer(sandbox=True,verbose=True,name='example_MTL',seed = 18012000,train_data = train_data,val_data=val1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 19:28:03,436 - INFO - Epoch 1: Train Loss: 0.3993, Val Loss: 0.3951\n",
      "2024-04-03 19:28:15,223 - INFO - Epoch 2: Train Loss: 0.3936, Val Loss: 0.3904\n",
      "2024-04-03 19:28:27,074 - INFO - Epoch 3: Train Loss: 0.3895, Val Loss: 0.3903\n",
      "2024-04-03 19:28:38,885 - INFO - Epoch 4: Train Loss: 0.3868, Val Loss: 0.3882\n",
      "2024-04-03 19:28:50,763 - INFO - Epoch 5: Train Loss: 0.3843, Val Loss: 0.3819\n",
      "2024-04-03 19:29:02,660 - INFO - Epoch 6: Train Loss: 0.3811, Val Loss: 0.3800\n",
      "2024-04-03 19:29:14,498 - INFO - Epoch 7: Train Loss: 0.3786, Val Loss: 0.3811\n",
      "2024-04-03 19:29:26,409 - INFO - Epoch 8: Train Loss: 0.3764, Val Loss: 0.3818\n",
      "2024-04-03 19:29:38,372 - INFO - Epoch 9: Train Loss: 0.3741, Val Loss: 0.3777\n",
      "2024-04-03 19:29:50,302 - INFO - Epoch 10: Train Loss: 0.3729, Val Loss: 0.3789\n",
      "2024-04-03 19:30:02,227 - INFO - Epoch 11: Train Loss: 0.3706, Val Loss: 0.3728\n",
      "2024-04-03 19:30:14,060 - INFO - Epoch 12: Train Loss: 0.3685, Val Loss: 0.3730\n",
      "2024-04-03 19:30:25,966 - INFO - Epoch 13: Train Loss: 0.3667, Val Loss: 0.3718\n",
      "2024-04-03 19:30:37,901 - INFO - Epoch 14: Train Loss: 0.3642, Val Loss: 0.3716\n",
      "2024-04-03 19:30:49,818 - INFO - Epoch 15: Train Loss: 0.3634, Val Loss: 0.3690\n",
      "2024-04-03 19:31:01,709 - INFO - Epoch 16: Train Loss: 0.3620, Val Loss: 0.3713\n",
      "2024-04-03 19:31:13,527 - INFO - Epoch 17: Train Loss: 0.3600, Val Loss: 0.3682\n",
      "2024-04-03 19:31:25,398 - INFO - Epoch 18: Train Loss: 0.3585, Val Loss: 0.3687\n",
      "2024-04-03 19:31:37,233 - INFO - Epoch 19: Train Loss: 0.3573, Val Loss: 0.3677\n",
      "2024-04-03 19:31:49,050 - INFO - Epoch 20: Train Loss: 0.3557, Val Loss: 0.3677\n",
      "2024-04-03 19:32:00,850 - INFO - Epoch 21: Train Loss: 0.3543, Val Loss: 0.3690\n",
      "2024-04-03 19:32:12,644 - INFO - Epoch 22: Train Loss: 0.3527, Val Loss: 0.3670\n",
      "2024-04-03 19:32:24,603 - INFO - Epoch 23: Train Loss: 0.3507, Val Loss: 0.3667\n",
      "2024-04-03 19:32:36,128 - INFO - Epoch 24: Train Loss: 0.3503, Val Loss: 0.3665\n",
      "2024-04-03 19:32:47,415 - INFO - Epoch 25: Train Loss: 0.3482, Val Loss: 0.3668\n",
      "2024-04-03 19:32:58,685 - INFO - Epoch 26: Train Loss: 0.3470, Val Loss: 0.3697\n",
      "2024-04-03 19:33:09,940 - INFO - Epoch 27: Train Loss: 0.3461, Val Loss: 0.3685\n",
      "2024-04-03 19:33:21,222 - INFO - Epoch 28: Train Loss: 0.3433, Val Loss: 0.3642\n",
      "2024-04-03 19:33:32,603 - INFO - Epoch 29: Train Loss: 0.3413, Val Loss: 0.3620\n",
      "2024-04-03 19:33:43,936 - INFO - Epoch 30: Train Loss: 0.3402, Val Loss: 0.3627\n",
      "2024-04-03 19:33:55,222 - INFO - Epoch 31: Train Loss: 0.3387, Val Loss: 0.3634\n",
      "2024-04-03 19:34:06,569 - INFO - Epoch 32: Train Loss: 0.3376, Val Loss: 0.3628\n",
      "2024-04-03 19:34:17,840 - INFO - Epoch 33: Train Loss: 0.3354, Val Loss: 0.3617\n",
      "2024-04-03 19:34:29,225 - INFO - Epoch 34: Train Loss: 0.3328, Val Loss: 0.3624\n",
      "2024-04-03 19:34:40,514 - INFO - Epoch 35: Train Loss: 0.3314, Val Loss: 0.3626\n",
      "2024-04-03 19:34:51,906 - INFO - Epoch 36: Train Loss: 0.3301, Val Loss: 0.3652\n",
      "2024-04-03 19:35:03,162 - INFO - Epoch 37: Train Loss: 0.3274, Val Loss: 0.3613\n",
      "2024-04-03 19:35:14,500 - INFO - Epoch 38: Train Loss: 0.3256, Val Loss: 0.3623\n",
      "2024-04-03 19:35:25,826 - INFO - Epoch 39: Train Loss: 0.3244, Val Loss: 0.3698\n",
      "2024-04-03 19:35:37,178 - INFO - Epoch 40: Train Loss: 0.3229, Val Loss: 0.3611\n",
      "2024-04-03 19:35:48,535 - INFO - Epoch 41: Train Loss: 0.3214, Val Loss: 0.3643\n",
      "2024-04-03 19:35:59,931 - INFO - Epoch 42: Train Loss: 0.3203, Val Loss: 0.3664\n",
      "2024-04-03 19:36:11,215 - INFO - Epoch 43: Train Loss: 0.3187, Val Loss: 0.3659\n",
      "2024-04-03 19:36:22,469 - INFO - Epoch 44: Train Loss: 0.3167, Val Loss: 0.3616\n",
      "2024-04-03 19:36:33,842 - INFO - Epoch 45: Train Loss: 0.3142, Val Loss: 0.3629\n",
      "2024-04-03 19:36:45,226 - INFO - Epoch 46: Train Loss: 0.3137, Val Loss: 0.3653\n",
      "2024-04-03 19:36:56,479 - INFO - Epoch 47: Train Loss: 0.3102, Val Loss: 0.3620\n",
      "2024-04-03 19:37:07,633 - INFO - Epoch 48: Train Loss: 0.3087, Val Loss: 0.3635\n",
      "2024-04-03 19:37:18,846 - INFO - Epoch 49: Train Loss: 0.3068, Val Loss: 0.3632\n",
      "2024-04-03 19:37:30,061 - INFO - Epoch 50: Train Loss: 0.3043, Val Loss: 0.3631\n",
      "2024-04-03 19:37:41,281 - INFO - Epoch 51: Train Loss: 0.3040, Val Loss: 0.3644\n",
      "2024-04-03 19:37:41,281 - INFO - Early stopping at epoch 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.3993008751088417,\n",
       "  0.39363831665651405,\n",
       "  0.3894823925042394,\n",
       "  0.3867871915940781,\n",
       "  0.38432445703548146,\n",
       "  0.3810740985505234,\n",
       "  0.3785891310287186,\n",
       "  0.37637332737462215,\n",
       "  0.37407389818522374,\n",
       "  0.37286512357361,\n",
       "  0.3705648761312947,\n",
       "  0.3684971938443185,\n",
       "  0.366702665976772,\n",
       "  0.3642492652934709,\n",
       "  0.3633779171208335,\n",
       "  0.3620238487840879,\n",
       "  0.359972994765891,\n",
       "  0.3585025453968112,\n",
       "  0.357333280454766,\n",
       "  0.3557457990312091,\n",
       "  0.3542544731232333,\n",
       "  0.35266086251268725,\n",
       "  0.3506674230326702,\n",
       "  0.3502553290204445,\n",
       "  0.3481828979956818,\n",
       "  0.34702472450536637,\n",
       "  0.3461174216723787,\n",
       "  0.34325734044666834,\n",
       "  0.34125981243889797,\n",
       "  0.34019968095475,\n",
       "  0.33865343518573393,\n",
       "  0.33762982097050753,\n",
       "  0.33538475632042036,\n",
       "  0.332835063500014,\n",
       "  0.33142944812002156,\n",
       "  0.3301370630791964,\n",
       "  0.3274308428458328,\n",
       "  0.32564903047898025,\n",
       "  0.32443453142222156,\n",
       "  0.3228741912729816,\n",
       "  0.3214330656427276,\n",
       "  0.32034784816324735,\n",
       "  0.3186778617180972,\n",
       "  0.3166617154565943,\n",
       "  0.3141588541346779,\n",
       "  0.31371226489455084,\n",
       "  0.3102001493591543,\n",
       "  0.3087327095769545,\n",
       "  0.3068463608877318,\n",
       "  0.3042718468358685,\n",
       "  0.30398568902847745],\n",
       " [0.39514843675501954,\n",
       "  0.39035992136510966,\n",
       "  0.39027048292181865,\n",
       "  0.3882146393490565,\n",
       "  0.38192779321407583,\n",
       "  0.3799789658002752,\n",
       "  0.38109163311769456,\n",
       "  0.38177842463433265,\n",
       "  0.3776772303089953,\n",
       "  0.3789405390837813,\n",
       "  0.37278396738575675,\n",
       "  0.3729540952499468,\n",
       "  0.3718158778445894,\n",
       "  0.3716396762912804,\n",
       "  0.3689892710503381,\n",
       "  0.37129601857142996,\n",
       "  0.3681853336566437,\n",
       "  0.36873339128791055,\n",
       "  0.36766984115812307,\n",
       "  0.3676929307688196,\n",
       "  0.3690439141464162,\n",
       "  0.36704615716534794,\n",
       "  0.3667010523197077,\n",
       "  0.3664779998208373,\n",
       "  0.36684058198267566,\n",
       "  0.3696790022451705,\n",
       "  0.3684674656364512,\n",
       "  0.36421600584763053,\n",
       "  0.3620265666520759,\n",
       "  0.36274856835591507,\n",
       "  0.3633567903456734,\n",
       "  0.36284818660198626,\n",
       "  0.36166327877894566,\n",
       "  0.362390651571343,\n",
       "  0.3625874809918985,\n",
       "  0.3651658580627227,\n",
       "  0.36127543970231457,\n",
       "  0.36230059511534163,\n",
       "  0.3697831862600179,\n",
       "  0.3611424006806876,\n",
       "  0.36425447248836973,\n",
       "  0.36635928852632305,\n",
       "  0.36587854854114404,\n",
       "  0.3615547500478742,\n",
       "  0.36293601599677117,\n",
       "  0.36532728432470185,\n",
       "  0.3620192236594594,\n",
       "  0.36350680144653863,\n",
       "  0.36316228452115,\n",
       "  0.36310903619291857,\n",
       "  0.36437566677104305])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using default hyperparameters\n",
    "example_mtl_model.train_and_validate(num_epochs=100, save_models=True, es_patience=10, save_losses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(example_mtl_model.model,'model_VP_logP_test.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtl_dc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
